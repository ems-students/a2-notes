% Copyright (c) 2015 William Bevington, Callum O'Brien and Alex Pace

% Permission is granted to copy, distribute and/or modify this document under
% the terms of the GNU Free Documentation License, Version 1.3 or any later
% version published by the Free Software Foundation; with no Invariant Sections,
% no Front-Cover Texts, and no Back-Cover Texts.

\documentclass{article}

\usepackage{amsmath, amssymb, booktabs, geometry, marginnote, tikz, xfrac}

\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}

\begin{document}

\title{S3}
\author{Will Bevington \and Callum O'Brien \and Alex Pace}
\maketitle

\tableofcontents

\newpage

\section{Combining Random Variables}

Let $X$ and $Y$ be two independent random variables with means $\textrm{E}(X)$
and $\textrm{E}(Y)$ respectively, and variances $\textrm{Var}(X)$ and
$\textrm{Var}(Y)$ respectively;

\[\textrm{E}(X\pm Y)=(\textrm{E}X)\pm\textrm{E}(Y)\] \[\textrm{Var}(X\pm
Y)=\textrm{Var}(X)+\textrm{Var}(Y)\] \[\textrm{E}(aX\pm b)=a\textrm{E}(X)\pm b\]

\[\textrm{Var}(aX\pm b)=a^2\textrm{Var}(X)+b\]

\noindent The latter two formulae should be recalled from S1. We can combine
these to acquire:

\[\textrm{E}(aX\pm bY) = a\textrm{E}(X)\pm b\textrm{E}(y)\] 

\[\textrm{Var}(aX\pm
bY) = a^2\textrm{Var}(X)+b^2\textrm{Var}(Y)\]

\noindent Additionally, the combination of two independent normal distributions
is also a normal distribution;

\[X~N(\mu_x,\sigma_x^2)\] 

\[Y~N(\mu_y,\sigma_y^2)\] 

\[(aX\pm bY)~N(a\mu_x\pm
b\mu_y,a^2\sigma_x^2+b^2\sigma_y^2)\]

\section{Sampling Frames}

\begin{description}
    
    \item{Population} The whole set of items that are of interest.
    \item{Census} Observes or measures every memeber of a population.
    \item{Sample Survey} A selection of observations taken from a subset of the
        population which is used to find out information about the population as
        a whole.
    \item{Random Sample} A sample in which every possible sample of size $n$ has
        an equal chance of being selected.  
    \item{Sampling Frame} A list identifying every single sampling unit that
        could be included in the sample

\end{description}

\subsection{Random Sampling}

\subsubsection{Random Number Sample}

Give each sampling unit in the sampling frame a number and use a random number
generator or random number tables to select required number of sampling units.

\subsubsection{Lottery Sample}

Put the sampling units from the sampling frame into a "hat" and select randomly
without replacing.

\subsubsection{Positives}

\begin{itemize}

    \item Random and free from bias 
    \item Easy to carry out

\end{itemize}

\subsubsection{Negatives}

\begin{itemize}

    \item Not suitable for large sample sizes 
\end{itemize}

\subsection{Systematic Sampling}

Pick at required intervals from an ordered list, e.g. I want a sample of 15 from
60: $\frac{60}{15}=4$ therefore choose a starting point randomly from one of the
first four sampling unit from the ordered list, then choose every fourth
sampling unit after until you have selected 15.

\subsubsection{Positives}
    
\begin{itemize}

    \item Suitable for large samples 
    \item Is easy to carry out

\end{itemize}

\subsubsection{Negatives}

\begin{itemize}

    \item Sample is not random unless the ordered list is random 
    \item Can introduce bias
    
\end{itemize}

\subsection{Stratified Sampling}

A form of random sampling: The population is split into mutually exclusive
groups (strata). Random samples are taken from each strata, the relative size of
each corresponds to the same ratio as each strata's representation in the total
population. \\\\

\subsubsection{Positives}

\begin{itemize}

    \item Works well with large samples that can be split into mutually
        exclusive groups 
    \item Reflects a populations structure
    
\end{itemize}

\subsubsection{Negatives}

\begin{itemize}

    \item Takes longer than random sampling 
    \item Within each strata the problems are the same as with any random sample.
    \item Ill defined strata can overlap (meaning they are no longer mutually
        exclusive) 
    \item Can't provide accurate data when strata overlap
    
\end{itemize}

\subsection{Quota Sampling}

When no sampling frame is available, quota sampling may be used. The population
is divided into groups (as with stratified sampling). Quotas for each group are
created that corrrespond with the groups representation in the total population.
The interviewer then selects sampling units until each quota is reached.

\subsubsection{Positives}

\begin{itemize}

    \item Administering the test is easy 
    \item Test is low cost 
    \item Test is quick if the sample is small
    
\end{itemize}

\subsubsection{Negatives}

\begin{itemize}

    \item Introduces interviewer bias 
    \item Can't estimate sampling errors
    
\end{itemize}

\section{Types of Data}

\subsection{Primary Data}

When you collect data, or someone collects data on your behalf.

\subsubsection{Positives}

\begin{itemize}

    \item You have control over the type and method of collection 
    \item The exact data needed is collected 
    \item The Accuracy is known

\end{itemize}

\subsubsection{Negatives}

\begin{itemize}

    \item Expensive (money and time)
    
\end{itemize}

\subsection{Seconday Data}

Second hand data, collected by another person or organisation.

\subsubsection{Positives}
        
\begin{itemize}

    \item Cheaper than gathering primary data (time and money) 
    \item Large amounts of data are easily available on the internet 
    \item Access to data over time (trends)
    
\end{itemize}

\subsubsection{Negatives}

\begin{itemize}

    \item Bias is not always acknowledged 
    \item Accuracy is not known 
    \item Certain data can be in a form that is difficult to deal with

\end{itemize}

\section{Estimating Population Parameters using a Sample}

A statistic which is used to estimate a population parameter is called an
estimator. A particular value is called an estimation. If $X$ is a random
variable then $\textrm{E}(X)$ would be an estimator of the mean. If a statistic
$T$ is an estimator for a population parameter $\theta$ and
$\textrm{E}(T)=\theta$ then $T$ is an unbiased estimator for $\theta$.
Otherwise, the bias of $T$ is given by the expression

\[\textrm{E}(T)-\theta\]

\noindent Estimators for population parameters can be written using ``hat
notation,'' wherein an estimator for a population parameter $\theta$ is denoted
by $\hat{\theta}$.

\begin{equation}\bar{X}=\frac{1}{n}\sum_iX_i\Rightarrow
\textrm{E}(\bar{X})=\mu_X\end{equation}
\begin{equation}S^2=\frac{1}{n-1}\left(\sum_iX_i^2-n\bar{X}^2\right)\Rightarrow
\textrm{E}(S^2)=\sigma_X^2\end{equation}

\paragraph{Proof of (1)} assuming $E(X+Y)=E(X)+E(Y)$,

\[E(\bar{X})=E\left(\frac{1}{n}\sum_iX_i\right)=\frac{1}{n}E\left(\sum_iX_i\right)\]

\[E(\bar{X})=\frac{1}{n}\sum_iE(X_i)=\frac{1}{n}n\mu=\mu\]

\subsection{Standard Error}

If $\bar{x}$ is an estimator of the mean, $\frac{\sigma}{\sqrt n}$ is the
standard error. As we probably don't know $\sigma$, use $S$ instead
$\left(\frac{S}{\sqrt n}\right)$. Note that as $n$ increases, the standard error
decreases.

\subsection{The Central Limit Theorem}

The central limit theorem states that if $X_1,X_2,\cdots,X_n$ is a random sample
of size $n$ from population with mean $\mu$ and variance $\sigma^2$ and $n$ is
large, then $\bar{X} \sim N \left(\mu,\frac{\sigma^2}{n}\right)$.

\subsubsection{Worked Example}

\paragraph{Question:} A die $\left\{1, 1, 1, 3, 3, 6\right\}$ is rolled 40 times 
and the mean of 40 rolls is calculated. Find an approximation for the
probability that $\textit{mean}>3$.

\paragraph{Answer:}
\[\begin{array}{cccc}
    x      & 1           & 3           & 6 \\
    P(X=x) & \frac{1}{2} & \frac{1}{3} & \frac{1}{6} \\
\end{array}\]

\[E(X)=\sum x P(X=x) = \frac{5}{2}\]

\[E(X^2)=\sum x^2 P(X=x) = \frac{19}{2}\]

\[\therefore Var(X) = \frac{19}{2} - \left(\frac{5}{2}\right)^2 = \frac{38 -
25}{4} = \frac{13}{4}\]

\noindent Thus, by the central limit theorem,

\[\bar{X} \sim N\left(\frac{5}{2}, \frac{13}{4} \times \frac{1}{40}\right)\]
    
\[\sim N\left(2.5, 0.08125\right)\]

\[P\left(\bar{X} > 3\right) = P\left(Z > \frac{3 - 2.5}{\sqrt{0.08125}}\right) =
P\left(Z > 1.7184\right) = 0.0397\]

\subsection{Calculating Confidence Intervals for a Population Interval}

Typical confidence levels are $99\%$ or $95\%$. A $95\%$ confidence interval is
the range of outcomes that $95\%$ of your results will fall in. A $95\%$
confidence interval for $\mu$ from a sample of size $n$ from a population with
mean $\mu$ and variance $\sigma^2$ is:

\[\bar{X} \pm 1.96 \times \frac{\sigma}{\sqrt{n}}\]

\noindent A $99\%$ confidence interval of $\mu$ is:

\[\bar{X} \pm 2.58 \times \frac{\sigma}{\sqrt{n}}\]

\begin{figure}
    
    \begin{tikzpicture}

    \end{tikzpicture}

\end{figure}

\section{Testing Hypotheses}

\subsection{Differences of Means of two Independent Normal Distributions}

\[X \sim N\left(\mu_X, \left(\sigma_X\right)^2\right)\]

\[Y \sim N\left(\mu_Y, \left(\sigma_Y\right)^2\right)\]

\[X - Y \sim N\left(\mu_X - \mu_Y, \left(\sigma_X\right)^2 +
\left(\sigma_Y\right)^2\right)\]

\noindent Taking a sample of $n_X$ from $X$ and $n_Y$ from $Y$ to get $\bar{X},
\bar{Y}$,

\begin{equation}\bar{X} - \bar{Y} \sim N\left(\mu_X - \mu_Y,
        \frac{\left(\sigma_X\right)^2}{n_X} +
\frac{\left(\sigma_Y\right)^2}{n_Y}\right)\end{equation}

\marginnote{If $X$ and $Y$ weren't normally distributed, (3) would still be a
good approximation if $n_X$ and $n_Y$ were large (by the central limit
theorem.)}

\noindent This gives the test statistic,

\[Z = \frac{\left(\bar{X} - \bar{Y}\right) - \left(\mu_X - \mu_Y\right)}
{\sqrt{\frac{\left(\sigma_X\right)^2}{n_X} +
\frac{\left(\sigma_Y\right)^2}{n_Y}}}\]

\subsubsection{Example}

\[H_0:\, \mu_X = \mu_Y,\; H_1:\, \mu_X > \mu_Y,\; \alpha = 0.05\]

\[\begin{array}{ccc}
    
                         & X  & Y \\
    \textit{sample mean} & 48 & 45 \\
    \sigma               & 5  & 8 \\
    n                    & 25 & 30 \\

\end{array}\]

\[Z = \frac{\bar{X} - \bar{Y} - (0)}{\sqrt{\frac{25}{25} + \frac{64}{30}}} =
\frac{3}{\sqrt{3.1333}} = 1.6947\]

\[P\left(Z < a\right) = 0.95 \Rightarrow a = 1.6449,\; 1.6947 > 1.6449\]

\noindent $\therefore$ There is sufficient evidence to reject $H_0$ in favour of
$H_1$

\section{Goodness of Fit \& the $\chi^2$ Distribution}

\[X^2 = \sum_i \frac{\left(O_i - E_i\right)^2}{E_i}\] \[= \sum_i
\frac{\left(O_i\right)^2 - 2O_iE_i + \left(E_i\right)^2}{E_i}\] \[= \sum_i
\left(\frac{\left(O_i\right)^2}{E_i} - 2O_i + E_i\right)\] \[= \sum_i
\frac{\left(O_i\right)^2}{E_i} - 2\sum_iO_i + \sum_iE_i\] \[\sum_i O_i = \sum_i
E_i = N\] hence \[X^2 = \sum_i \frac{\left(O_i\right)^2}{E_i} - N\] where
\[N = \textit{N.o. cells}\] \marginnote{$X^2$ is approximated well by $\chi^2$
if none of the expected values fall below five.}

\subsection{Degrees of Freedom}

\subsection{Binomial Distribution}

If any $E_i < 5$, these cells, along with the corresponding $O_i$ must be merged
to avoid this. Then, \[\textit{N.o. degrees of freedom} = N - 2\] if $p$ is
estimated by calculation and \[\textit{N.o. degrees of freedom} = N - 1\]
otherwise.

\subsection{Poisson Distribution}

\subsection{Uniform Continuous Distribution}

\subsection{Normal Distribution}

We may suspect that some data are normally distributed if they exhibit the
characteristic `bell-shaped' curve and/or if roughly $\sfrac{2}{3}$ of the data
lie within one standard deviation from the mean.

A sample from a population $X \sim N\left(\mu, \sigma^2\right)$ consisting of $n$
fields has $\nu$ degrees of freedom where \[\nu = 
\begin{cases} 
    n - 3 & \quad \text{if } \mu \text{ and } \sigma^2 \text{ are estimated} \\
    n - 2 & \quad \text{if } \mu \text{ or } \sigma^2 \text{ is estimated} \\
    n - 1 & \quad \text{otherwise} \\
\end{cases}\]

\subsubsection{Question (tell me what you think about me)}

During an observation on the height of \oldstylenums{200} male students the
following data were observed:\begin{table}[h!]

    \centering

    \begin{tabular}{lr}

        \toprule

        Height / cm & Frequency \\

        \midrule

        150-154     & 4 \\
        155-159     & 6 \\
        160-164     & 12 \\
        165-169     & 30 \\
        170-174     & 64 \\
        175-179     & 52 \\
        180-184     & 18 \\
        185-189     & 10 \\
        190-194     & 4 \\

        \bottomrule

    \end{tabular}

\end{table}\begin{enumerate}

    \item Test at the \oldstylenums{0.05} level to see if the height of male
        students could be modelled by a normal distribution with mean
        \oldstylenums{172} and standard deviation \oldstylenums{6}.

    \item Describe how you would modify this test if the mean and variance were
        unknown.

\end{enumerate}

\subsection{Contingency Tables}

A contingency table is a way to test whether two variables are independent or
linked. The null hypothesis is always that the variables are independent.

\subsubsection[Example]{Example: Link between School and Grade}

\begin{table}[h]

    \centering

    \begin{tabular}{lllll}

        \toprule

        Grade       & A     & B     & C     & Total \\

        \midrule

        School X    & 18    & 12    & 20    & 50 \\
        School Y    & 26    & 12    & 32    & 70 \\
        Total       & 44    & 24    & 52    & 120 \\

        \bottomrule

    \end{tabular}

    \caption{Observed values}

\end{table}

\[H_0 : \textrm{ School and grade are independent.}\]
\[H_1 : \textrm{ School and grade are linked.}\]

Assuming $H_0$, we can calculate expected values by multiplying relevant
probabilities and quantities, as \[A \textrm{ and } B \textrm{ are independent}
\Leftrightarrow P(A \land B) = P(A) \times P(B)\] giving the following expected
values:

\begin{table}[h]

    \centering

    \begin{tabular}{llll}

        \toprule

        Grade       & A     & B     & C \\

        \midrule

        School X    & 18.33 & 10.00 & 21.67 \\
        School Y    & 25.67 & 14.00 & 30.33 \\

        \bottomrule

    \end{tabular}

    \caption{Expected Values}

\end{table}

hence, for these data, \[X^2 = 0.92\]

As these data have two degrees of freedom, the critical value (at significance
\oldstylenums{0.005}) is given by \[\chi^2_2\left(0.05\right) = 5.99\] which is
greater than $X^2$. There is therefore insufficient evidence to reject $H_0$,
i.e. school and grade are independent.

\end{document}

